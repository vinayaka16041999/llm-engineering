{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable, Dict, Any, get_type_hints\n",
    "from inspect import signature\n",
    "from openai import Client\n",
    "from dotenv import load_dotenv\n",
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_weather(loc:str,units:str)->str:\n",
    "    if loc=='Hyd':\n",
    "        return \"25\" +f\" degrees {units}\"\n",
    "    else:\n",
    "        return \"50\" +f\" degrees {units}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add(a:int,b:int):\n",
    "    return a+b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "def say_hello():\n",
    "    print(\"Hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tool:\n",
    "    def __init__(self):\n",
    "        self.tools:Dict = {}\n",
    "\n",
    "    def register(self, func:Callable)->None:\n",
    "        fn_signature = self.get_fn_signature(func)\n",
    "        self.tools[fn_signature['name']] = func\n",
    "        print(\"Tool Registered Successfully\")\n",
    "    \n",
    "    def view_tools(self):\n",
    "        print(self.tools)\n",
    "\n",
    "    def get_fn_signature(self, fn: Callable) -> Dict[str, Any]:\n",
    "        fn_signature = {\n",
    "            \"name\": fn.__name__,\n",
    "            \"description\": fn.__doc__ or \"No description available.\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {},\n",
    "                \"required\": []\n",
    "            }\n",
    "        }\n",
    "\n",
    "        # Get parameter hints (handles missing annotations)\n",
    "        type_hints = get_type_hints(fn)\n",
    "\n",
    "        \n",
    "        sig = signature(fn)\n",
    "\n",
    "        for param_name, param in sig.parameters.items():\n",
    "            if param_name == \"return\":\n",
    "                continue  # Skip return type\n",
    "            \n",
    "            # Try to get the type, otherwise default to \"any\"\n",
    "            param_type = type_hints.get(param_name, \"any\")\n",
    "            param_type = param_type.__name__ if hasattr(param_type, \"__name__\") else \"any\"\n",
    "            \n",
    "            fn_signature[\"parameters\"][\"properties\"][param_name] = {\"type\": param_type}\n",
    "            fn_signature[\"parameters\"][\"required\"].append(param_name)\n",
    "\n",
    "        return fn_signature\n",
    "\n",
    "\n",
    "    def tool_call(self, prompt):\n",
    "        client = Client()\n",
    "        \n",
    "        function_definitions = \"\\n\\n\".join([\n",
    "            f\"Function Name: {fn_name}\\nFunction Description: {func.__doc__ or 'No description available.'}\\n\"\n",
    "            f\"Function Signature: {self.get_fn_signature(func)['parameters']}\"\n",
    "            for fn_name, func in self.tools.items()\n",
    "        ])\n",
    "\n",
    "        \n",
    "        system_message = f\"\"\"You are an helpful assistant who converts the user's query into function call if the user query needs the use of the function based on the given function definition so that the tool can be triggered and the context can be passed to LLM.\\nBelow are the function definition:\\n\\n{function_definitions}\n",
    "            \"\"\"\n",
    "        \n",
    "        system_message += \"\"\"\\nThe output should be a Dictionary in the below format:\\n{\"name\": \"get_weather\", \"args\": {\"loc\": \"Hyd\", \"units\":\"C\"}}\"\"\"\n",
    "        \n",
    "        response = client.chat.completions.create(\n",
    "            model = \"gpt-4o-mini\",\n",
    "            messages = [{'role':'system','content':system_message},\n",
    "                        {'role':'user','content':f'For the given user query choose the best function and give me the function call as dict\\nQuery: {prompt}'}]\n",
    "        )\n",
    "        \n",
    "        return response.choices[0].message.content\n",
    "    def call_function(self, prompt):\n",
    "        function_call = self.tool_call(prompt)\n",
    "        function_call = ast.literal_eval(function_call)\n",
    "        function = self.tools[function_call['name']]\n",
    "        kwargs = function_call['args']\n",
    "        return function(**kwargs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tool Registered Successfully\n",
      "Tool Registered Successfully\n",
      "Tool Registered Successfully\n"
     ]
    }
   ],
   "source": [
    "tools = Tool()\n",
    "tools.register(get_weather)\n",
    "tools.register(add)\n",
    "tools.register(say_hello)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LLM:\n",
    "    def __init__(self, Tool):\n",
    "        self.tool = Tool\n",
    "    \n",
    "    def chat_with_tools(self, query):\n",
    "        context = self.tool.call_function(query)\n",
    "        print(context)\n",
    "        client = Client()\n",
    "        messages = [\n",
    "            {'role':'system','content':f'The context below contains the answer to the users query. Use it to generate appropriate answer to the users query\\n{context}'},\n",
    "            {'role':'user','content':query}\n",
    "        ]\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=messages\n",
    "        )\n",
    "        return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-9\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The addition of 10 and -19 is -9.'"
      ]
     },
     "execution_count": 286,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm = LLM(tools)\n",
    "llm.chat_with_tools(\"What is the addition of 10 and -19\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
